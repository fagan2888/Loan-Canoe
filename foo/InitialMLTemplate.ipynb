{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This is a preliminary ML script to code Naive Bayes and Logistic Regression models for our project.**\n",
    "\n",
    "Naive Bayes Overview:\n",
    "\"...if we make very naive assumptions about the generative model for each label, we can find a rough approximation of the generative model for each class, and then proceed with the Bayesian classification.  Different types of naive Bayes classifiers rest on different naive assumptions about the data...\" (VanderPlas, Jake.  Python Data Science Handbook.  O'Reilly Media, Inc.: 2016.\n",
    "\n",
    "Description of Target(replace with .gov source in final notebook, this is enough to get started though): \n",
    "https://regulatorysol.com/action-taken-action-taken-date/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#delete this in a future version of the notebook\n",
    "root = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import 2017 sample of 25,000 observations.**  Note import warning:\"Columns (29,30,39,40) have mixed types. Specify dtype option on import or set low_memory=False.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Fetch the data if required\n",
    "filepath = os.path.abspath(os.path.join( \"..\", \"fixtures\", \"hmda2017sample.csv\"))\n",
    "DATA = pd.read_csv(filepath)\n",
    "DATA.describe(include='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Write the initial script using subset of features which are already int or float, plus the target** Future version of script will address full set of features, and will move away from use of the lambda function for readability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA['action_taken'] = DATA.action_taken_name.apply(lambda x: 1 if x in ['Loan purchased by the institution', 'Loan originated'] else 0)\n",
    "pd.crosstab(DATA['action_taken_name'],DATA['action_taken'], margins=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA = DATA[['tract_to_msamd_income', \n",
    "            'population', \n",
    "            'minority_population', \n",
    "            'number_of_owner_occupied_units', \n",
    "            'number_of_1_to_4_family_units', \n",
    "            'loan_amount_000s', \n",
    "            'hud_median_family_income', \n",
    "            'applicant_income_000s', \n",
    "            'action_taken']]\n",
    "DATA.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TO DO: fix column [0]\n",
    "tofilepath = os.path.abspath(os.path.join( \"..\", \"fixtures\", \"hmda2017sample_test.csv\"))\n",
    "DATA.to_csv(tofilepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURES  = [\n",
    "    'tract_to_msamd_income', \n",
    "    'population', \n",
    "    'minority_population', \n",
    "    'number_of_owner_occupied_units', \n",
    "    'number_of_1_to_4_family_units', \n",
    "    'loan_amount_000s', \n",
    "    'hud_median_family_income', \n",
    "    'applicant_income_000s', \n",
    "    'action_taken'\n",
    "]\n",
    "\n",
    "ACTION_TAKEN_MAP = {\n",
    "    1: \"originated or purchased\",\n",
    "    0: \"other\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the shape of the data\n",
    "print(\"{} instances with {} features\\n\".format(*DATA.shape))\n",
    "\n",
    "# Determine the frequency of each class\n",
    "print(pd.crosstab(index=DATA['action_taken'], columns=\"count\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Stage the data for ML algorithms.** Need to determine whether we can keep y as binary or if it in fact has to be labeled for Scikit-Learn, Yellowbrick et al to work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Extract our X and y data\n",
    "X = DATA[FEATURES[:-1]]\n",
    "y = DATA['action_taken']\n",
    "\n",
    "# Encode our target variable\n",
    "encoder = LabelEncoder().fit(y)\n",
    "y = encoder.transform(y)\n",
    "\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Create a scatter matrix of the dataframe features\n",
    "from pandas.plotting import scatter_matrix\n",
    "scatter_matrix(X, alpha=0.2, figsize=(12, 12), diagonal='kde')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Extraction \n",
    "\n",
    "One way that we can structure our data for easy management is to save files on disk. The Scikit-Learn datasets are already structured this way, and when loaded into a `Bunch` (a class imported from the `datasets` module of Scikit-Learn) we can expose a data API that is very familiar to how we've trained on our toy datasets in the past. A `Bunch` object exposes some important properties:\n",
    "\n",
    "- **data**: array of shape `n_samples` * `n_features`\n",
    "- **target**: array of length `n_samples`\n",
    "- **feature_names**: names of the features\n",
    "- **target_names**: names of the targets\n",
    "- **filenames**: names of the files that were loaded\n",
    "- **DESCR**: contents of the readme\n",
    "\n",
    "**Note**: This does not preclude database storage of the data, in fact - a database can be easily extended to load the same `Bunch` API. Simply store the README and features in a dataset description table and load it from there. The filenames property will be redundant, but you could store a SQL statement that shows the data load. \n",
    "\n",
    "In order to manage our data set _on disk_, we'll structure our data as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets.base import Bunch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE: np.genfromtxt and objects for extracting the target from the data have syntactical issues**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def load_data(root=root):\n",
    "    # Construct the `Bunch` for the HMDA dataset\n",
    "    filenames     = {\n",
    "        'meta': os.path.join(root, 'fixtures','hmdameta.json'),\n",
    "        'rdme': os.path.join(root, 'fixtures','hmdareadme.txt'),\n",
    "        'data': os.path.join(root, 'fixtures','hmda2017sample_test.csv'),\n",
    "    }\n",
    "\n",
    "    # Load the meta data from the meta json\n",
    "    with open(filenames['meta'], 'r') as f:\n",
    "        meta = json.load(f)\n",
    "        target_names  = meta['target_names']\n",
    "        feature_names = meta['feature_names']\n",
    "\n",
    "    # Load the description from the README. \n",
    "    with open(filenames['rdme'], 'r') as f:\n",
    "        DESCR = f.read()\n",
    "\n",
    "    # Load the dataset from the text file.\n",
    "    dataset = np.genfromtxt(filenames['data'], delimiter = \",\", names = True)\n",
    "\n",
    "    # Extract the target from the data\n",
    "    data   = dataset[:, 0:7]\n",
    "    target = dataset[:, -1]\n",
    "\n",
    "    # Create the bunch object\n",
    "    return Bunch(\n",
    "        data=data,\n",
    "        target=target,\n",
    "        filenames=filenames,\n",
    "        target_names=target_names,\n",
    "        feature_names=feature_names,\n",
    "        DESCR=DESCR\n",
    "    )\n",
    "\n",
    "# Save the dataset as a variable we can use.\n",
    "dataset = load_data()\n",
    "\n",
    "print(dataset.data.shape)\n",
    "print(dataset.target.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification \n",
    "\n",
    "Now that we have a dataset `Bunch` loaded and ready, we can begin the classification process. Let's attempt to build a classifier with kNN, SVM, and Random Forest classifiers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_and_evaluate(dataset, model, label, **kwargs):\n",
    "    \"\"\"\n",
    "    Because of the Scikit-Learn API, we can create a function to\n",
    "    do all of the fit and evaluate work on our behalf!\n",
    "    \"\"\"\n",
    "    start  = time.time() # Start the clock! \n",
    "    scores = {'precision':[], 'recall':[], 'accuracy':[], 'f1':[]}\n",
    "    \n",
    "    kf = KFold(n_splits = 12, shuffle=True)\n",
    "    \n",
    "    for train, test in kf.split(dataset.data):\n",
    "        X_train, X_test = dataset.data[train], dataset.data[test]\n",
    "        y_train, y_test = dataset.target[train], dataset.target[test]\n",
    "        \n",
    "        estimator = model(**kwargs)\n",
    "        estimator.fit(X_train, y_train)\n",
    "        \n",
    "        expected  = y_test\n",
    "        predicted = estimator.predict(X_test)\n",
    "        \n",
    "        # Append our scores to the tracker\n",
    "        scores['precision'].append(metrics.precision_score(expected, predicted, average=\"weighted\"))\n",
    "        scores['recall'].append(metrics.recall_score(expected, predicted, average=\"weighted\"))\n",
    "        scores['accuracy'].append(metrics.accuracy_score(expected, predicted))\n",
    "        scores['f1'].append(metrics.f1_score(expected, predicted, average=\"weighted\"))\n",
    "\n",
    "    # Report\n",
    "    print(\"Build and Validation of {} took {:0.3f} seconds\".format(label, time.time()-start))\n",
    "    print(\"Validation scores are as follows:\\n\")\n",
    "    print(pd.DataFrame(scores).mean())\n",
    "    \n",
    "    # Write official estimator to disk\n",
    "    estimator = model(**kwargs)\n",
    "    estimator.fit(dataset.data, dataset.target)\n",
    "    \n",
    "    outpath = label.lower().replace(\" \", \"-\") + \".pickle\"\n",
    "    with open(outpath, 'wb') as f:\n",
    "        pickle.dump(estimator, f)\n",
    "\n",
    "    print(\"\\nFitted model written to:\\n{}\".format(os.path.abspath(outpath)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform Gaussian Naive Bayes\n",
    "# need to try this out and extend to MultinomialNB, introducing Pipeline\n",
    "fit_and_evaluate(dataset, GaussianNB, \"Gaussian Naive Bayes\",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform Logistic Regression \n",
    "fit_and_evaluate(dataset, LogisticRegression, \"Logistic Regression\", )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
